{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ca2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.fft import fft, ifft, fftfreq\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import MonthBegin\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76a37de",
   "metadata": {},
   "source": [
    "## Hard-Coded Variables \n",
    "- uses domain knowledge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hardcoded information\n",
    "expected_groups = {\n",
    "    ('A', 7),\n",
    "    ('A', 8),\n",
    "    ('B', 1),\n",
    "    ('B', 2),\n",
    "    ('C', 3),\n",
    "    ('C', 4),\n",
    "    ('D', 5),\n",
    "    ('D', 6),\n",
    "}\n",
    "\n",
    "group_1 = {1,2,7,8}\n",
    "group_2 = {3,4,5,6}\n",
    "group_1_feature_bounds = {\n",
    "    \"wheeldiameter\": (29.0, 41),\n",
    "    \"wheelwidth\": (5.50, 5.9),\n",
    "    \"flangeheight\": (0.90, 1.55),\n",
    "    \"flangethickness\": (0.85, 1.75),\n",
    "    \"flangeslope\": (0.10, 0.60),\n",
    "    \"treadhollow\": (0.01, 5.3),\n",
    "    \"rimthickness\": (0.75, 1.85),\n",
    "    \"backtobackgauge\": (52.0, 53.0),\n",
    "}\n",
    "group_2_feature_bounds = {\n",
    "    \"wheeldiameter\": (29.0, 46.0),\n",
    "    \"wheelwidth\": (5.50, 5.87),\n",
    "    \"flangeheight\": (0.90, 1.55),\n",
    "    \"flangethickness\": (0.85, 1.75),\n",
    "    \"flangeslope\": (0.10, 0.60),\n",
    "    \"treadhollow\": (0.01, 5.1),\n",
    "    \"rimthickness\": (0.80, 2.1),\n",
    "    \"backtobackgauge\": (52.0, 53.0),\n",
    "}\n",
    "\n",
    "group_2_severity_thresholds = {'backtobackgauge': [(52.0, -5), (52.0, -4), (52.0, -3), (52.0, -2), (52.0, -1),\n",
    "                     (53.0, 1), (53.0, 2), (53.0, 3), (53.0, 4), (53.0, 5)],\n",
    " 'flangeheight': [(1.009, -5), (1.027, -4), (1.036, -3), (1.041, -2),\n",
    "                  (1.057, -1), (1.467, 1), (1.487, 2), (1.492, 3), (1.5, 4),\n",
    "                  (1.508, 5)],\n",
    " 'flangeslope': [(0.119, -5), (0.122, -4), (0.124, -3), (0.126, -2),\n",
    "                 (0.132, -1), (0.505, 1), (0.528, 2), (0.535, 3), (0.544, 4),\n",
    "                 (0.556, 5)],\n",
    " 'flangethickness': [(0.878, -5), (0.897, -4), (0.911, -3), (0.924, -2),\n",
    "                     (0.969, -1), (1.544, 1), (1.572, 2), (1.582, 3),\n",
    "                     (1.597, 4), (1.62, 5)],\n",
    " 'rimthickness': [(0.852, -5), (0.868, -4), (0.88, -3), (0.889, -2), (0.92, -1),\n",
    "                  (1.983, 1), (2.012, 2), (2.021, 3), (2.032, 4), (2.05, 5)],\n",
    " 'treadhollow': [(0.01, -5), (0.01, -4), (0.01, -3), (0.01, -2), (0.01, -1),\n",
    "                 (4.029, 1), (4.369, 2), (4.473, 3), (4.601, 4), (4.769, 5)],\n",
    " 'wheeldiameter': [(29.0, -5), (29.134, -4), (30.0, -3), (30.63, -2),\n",
    "                   (32.756, -1), (43.622, 1), (44.331, 2), (44.488, 3),\n",
    "                   (44.724, 4), (45.118, 5)],\n",
    " 'wheelwidth': [(5.516, -5), (5.526, -4), (5.533, -3), (5.539, -2), (5.554, -1),\n",
    "                (5.812, 1), (5.831, 2), (5.837, 3), (5.843, 4), (5.852, 5)]}\n",
    "\n",
    "group_1_severity_thresholds = {\n",
    "    'backtobackgauge': [(52.0, -5), (52.0, -4), (52.0, -3), (52.0, -2), (52.0, -1),\n",
    "                     (53.0, 1), (53.0, 2), (53.0, 3), (53.0, 4), (53.0, 5)],\n",
    " 'flangeheight': [(1.03, -5), (1.044, -4), (1.051, -3), (1.058, -2),\n",
    "                  (1.072, -1), (1.5, 1), (1.515, 2), (1.52, 3), (1.525, 4),\n",
    "                  (1.533, 5)],\n",
    " 'flangeslope': [(0.124, -5), (0.128, -4), (0.13, -3), (0.132, -2), (0.139, -1),\n",
    "                 (0.52, 1), (0.543, 2), (0.55, 3), (0.559, 4), (0.572, 5)],\n",
    " 'flangethickness': [(0.879, -5), (0.895, -4), (0.909, -3), (0.92, -2),\n",
    "                     (0.96, -1), (1.575, 1), (1.604, 2), (1.615, 3), (1.63, 4),\n",
    "                     (1.653, 5)],\n",
    " 'rimthickness': [(0.778, -5), (0.796, -4), (0.81, -3), (0.822, -2),\n",
    "                  (0.866, -1), (1.694, 1), (1.726, 2), (1.736, 3), (1.749, 4),\n",
    "                  (1.768, 5)],\n",
    " 'treadhollow': [(0.01, -5), (0.01, -4), (0.01, -3), (0.01, -2), (0.01, -1),\n",
    "                 (4.196, 1), (4.53, 2), (4.636, 3), (4.757, 4), (4.938, 5)],\n",
    " 'wheeldiameter': [(30.394, -5), (30.709, -4), (30.945, -3), (31.102, -2),\n",
    "                   (31.575, -1), (38.976, 1), (39.528, 2), (39.764, 3),\n",
    "                   (40.0, 4), (40.236, 5)],\n",
    " 'wheelwidth': [(5.527, -5), (5.537, -4), (5.543, -3), (5.548, -2), (5.563, -1),\n",
    "                (5.812, 1), (5.833, 2), (5.839, 3), (5.845, 4), (5.853, 5)]\n",
    "}\n",
    "\n",
    "geometry_features = ['flangeheight', 'rimthickness', 'wheeldiameter', 'wheelwidth','flangethickness','flangeslope', 'backtobackgauge','treadhollow', 'flangeangle']  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a746c4dc",
   "metadata": {},
   "source": [
    "## Initializations\n",
    "- loading datasets\n",
    "- filtering invalid values and axles\n",
    "- merging failure dfs into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339acb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def load_data(path_prefix: str = '../Datasets') -> dict:\n",
    "    \"\"\"\n",
    "    Load all required datasets and fill missing failure reasons.\n",
    "    Returns a dict of DataFrames.\n",
    "    \"\"\"\n",
    "    files = {\n",
    "        'failure': 'FailureTable0723.csv',\n",
    "        'equipment': 'equipment_data_masked.csv',\n",
    "        'mileage': 'Mileage0723.csv',\n",
    "        'wpd': 'Wpd0723.csv',\n",
    "    }\n",
    "    dfs = {name: pd.read_csv(f\"{path_prefix}/{fname}\", engine='pyarrow') for name, fname in files.items()}\n",
    "    dfs['failure']['failurereason'] = dfs['failure']['failurereason'].fillna('not failed')\n",
    "    dfs['wpd']= dfs['wpd'][dfs['wpd']['traindate'] < '2024-12-01']\n",
    "    return dfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa71425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with failurereason, and applieddate and mileage data\n",
    "def fast_filter(df, name,valid_axles, feature_bounds):\n",
    "    before = df.shape\n",
    "    valid_trucks = {(truck) for (truck, axle) in expected_groups if axle in valid_axles}\n",
    "    valid_pairs = {(truck, axle) for (truck, axle) in expected_groups if axle in valid_axles}\n",
    "\n",
    "    if 'axle' not in df.columns:\n",
    "        mask = df['truck'].isin(valid_trucks)\n",
    "    else:\n",
    "        mask = [pair in valid_pairs for pair in zip(df['truck'], df['axle'])]\n",
    "\n",
    "    df_filtered = df[mask].copy()\n",
    "    df_filtered[geometry_features + ['trainspeed']] = df_filtered[geometry_features + ['trainspeed']].replace(0, np.nan)\n",
    "    for feature, (min_val, max_val) in feature_bounds.items():\n",
    "        if feature in df_filtered.columns:\n",
    "            too_low = df_filtered[feature] < (0.8 * min_val)\n",
    "            too_high = df_filtered[feature] > (1.2 * max_val)\n",
    "\n",
    "            df_filtered.loc[too_low | too_high, feature] = np.nan\n",
    "\n",
    "            df_filtered.loc[(df_filtered[feature] >= (0.8 * min_val)) & (df_filtered[feature] < min_val), feature] = min_val\n",
    "            df_filtered.loc[(df_filtered[feature] <= (1.2 * max_val)) & (df_filtered[feature] > max_val), feature] = max_val\n",
    "\n",
    "    after = df_filtered.shape\n",
    "    print(f\"{name}: before = {before}, after = {after}\")\n",
    "    return df_filtered.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def fast_filter_by_group(df, name):\n",
    "    dfs = df.copy()\n",
    "    print(f\"{name}: pre-filter shape: {dfs.shape}\")\n",
    "    df_group_1 = fast_filter(dfs, f\"{name} (group1)\", group_1, group_1_feature_bounds)\n",
    "    df_group_2 = fast_filter(dfs, f\"{name} (group2)\", group_2, group_2_feature_bounds)\n",
    "    df_combined = pd.concat([df_group_1, df_group_2], ignore_index=True)\n",
    "    print(f\"{name}: combined shape = {df_combined.shape}\")\n",
    "\n",
    "    return df_combined\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbafa1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "dfs = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wpd = fast_filter_by_group(dfs['wpd'], 'wpd')\n",
    "df_failure = dfs['failure']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5cef4",
   "metadata": {},
   "source": [
    "## Adding applieddates to wpd records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ef8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wpd['recordmonth'] = df_wpd['traindate'].values.astype('datetime64[M]')\n",
    "df_wpd['recordmonth_next'] = df_wpd['recordmonth'] + pd.DateOffset(months=1)\n",
    "\n",
    "# getting rid of duplicates due to vendornumbersuppliercode \n",
    "df_failure_temp = df_failure.sort_values('applieddate').drop_duplicates(['equipmentnumber', 'truck', 'axle', 'side', 'recordmonth'])\n",
    "df_failure_temp['recordmonth'] = pd.to_datetime(df_failure_temp['recordmonth']).values.astype('datetime64[M]')\n",
    "\n",
    "# merging current month's applieddate\n",
    "merge_keys = ['equipmentnumber', 'truck', 'axle', 'side', 'recordmonth']\n",
    "df_wpd = df_wpd.merge(\n",
    "    df_failure_temp[merge_keys + ['applieddate']],\n",
    "    on=merge_keys,\n",
    "    how='left'\n",
    ")\n",
    "df_wpd.rename(columns={'applieddate': 'applieddate_initial'}, inplace=True)\n",
    "\n",
    "# merging next month's applieddate \n",
    "merge_keys_next = ['equipmentnumber', 'truck', 'axle', 'side', 'recordmonth_next']\n",
    "df_wpd = df_wpd.merge(\n",
    "    df_failure_temp.rename(columns={'recordmonth': 'recordmonth_next', 'applieddate': 'applieddate_next'})[\n",
    "        merge_keys_next + ['applieddate_next']\n",
    "    ],\n",
    "    on=merge_keys_next,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# if next months applieddate is before my traindate, I am apart of next month's wheel\n",
    "df_wpd['applieddate'] = np.where(\n",
    "    df_wpd['traindate'] >= df_wpd['applieddate_next'],\n",
    "    df_wpd['applieddate_next'],\n",
    "    df_wpd['applieddate_initial']\n",
    ")\n",
    "\n",
    "df_wpd = df_wpd.drop(columns=['recordmonth_next','applieddate_next','applieddate_initial'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d766e6",
   "metadata": {},
   "source": [
    "## Make DF for each wheel\n",
    "- starts at the applieddate, ends at the day before the next applieddate\n",
    "- apply FFT, decay-weighted rolling mean, and a combination of the 2 as a polynomial trend to the DF \n",
    "- more details in the notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c831c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.fft import fft, ifft, fftfreq\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.signal import butter, filtfilt, firwin, lfilter\n",
    "from scipy.optimize import minimize_scalar, differential_evolution\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from statsmodels.tsa.statespace.structural import UnobservedComponents\n",
    "import cvxpy as cp\n",
    "\n",
    "def snr_gain(y, y_smooth, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Compute SNR gain in dB relative to baseline.\n",
    "    \n",
    "    ΔSNR = SNR_filtered - SNR_original\n",
    "    \n",
    "    where:\n",
    "        SNR_original = 10*log10(var(y) / var(y - mean(y)))\n",
    "        SNR_filtered = 10*log10(var(y) / var(y - y_smooth))\n",
    "    \n",
    "    y        : original signal (array-like)\n",
    "    y_smooth : smoothed/filtered signal (array-like)\n",
    "    eps      : small constant to avoid division by zero\n",
    "    \"\"\"\n",
    "    mask = ~np.isnan(y_smooth)\n",
    "    if mask.sum() < 3:\n",
    "        return np.nan\n",
    "\n",
    "    y = y[mask]\n",
    "    y_smooth = y_smooth[mask]\n",
    "\n",
    "    var_signal = np.var(y, ddof=1)\n",
    "\n",
    "    # Baseline \"noise\" = deviation from mean\n",
    "    var_noise_orig = np.var(y - np.mean(y), ddof=1)\n",
    "\n",
    "    # Filtered \"noise\" = residual after smoothing\n",
    "    var_noise_filt = np.var(y - y_smooth, ddof=1)\n",
    "\n",
    "    snr_orig = 10 * np.log10(var_signal / (var_noise_orig + eps)) if var_noise_orig > eps else -np.inf\n",
    "    snr_filt = 10 * np.log10(var_signal / (var_noise_filt + eps)) if var_noise_filt > eps else -np.inf\n",
    "\n",
    "    return snr_filt - snr_orig\n",
    "\n",
    "def hf_var_reduction(y, y_smooth):\n",
    "    \"\"\"High-frequency variance reduction (%)\"\"\"\n",
    "    mask = ~np.isnan(y_smooth)\n",
    "    if mask.sum() < 3:\n",
    "        return 0\n",
    "    residual_in = y[mask] - np.mean(y[mask])\n",
    "    residual_out = y[mask] - y_smooth[mask]\n",
    "    var_in = np.var(residual_in)\n",
    "    var_out = np.var(residual_out)\n",
    "    return 100 * (var_in - var_out) / var_in if var_in > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e5cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wpd['traindate'] = pd.to_datetime(df_wpd['traindate'])\n",
    "df_wpd['applieddate'] = pd.to_datetime(df_wpd['applieddate'])\n",
    "df_wpd = df_wpd.sort_values(by=['equipmentnumber', 'truck', 'axle', 'side', 'applieddate', 'traindate'])\n",
    "\n",
    "def interpolate_daily_wheels(df, n_jobs=-1):\n",
    "    group_cols = ['equipmentnumber', 'truck', 'axle', 'side']\n",
    "    grouped = df.groupby(group_cols)\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    # Prepare tasks: each task = (sub_df, start_date, end_date)\n",
    "    for group_keys, group_df in grouped:\n",
    "        subgroups = group_df.groupby('applieddate')\n",
    "        sub_keys_sorted = sorted(subgroups.groups.keys())\n",
    "\n",
    "        for i, adate in enumerate(sub_keys_sorted):\n",
    "            sub_df = subgroups.get_group(adate).copy()\n",
    "            start_date = sub_df['applieddate'].min()\n",
    "            if start_date < pd.to_datetime('2020-01-01'):\n",
    "                start_date = pd.to_datetime('2020-01-01')\n",
    "            if i + 1 < len(sub_keys_sorted):\n",
    "                next_applieddate = sub_keys_sorted[i + 1]\n",
    "                end_date = pd.to_datetime(next_applieddate) - pd.Timedelta(days=1)\n",
    "            else:\n",
    "                end_date = pd.NaT\n",
    "\n",
    "            tasks.append((sub_df, start_date, end_date))\n",
    "\n",
    "    # Wrap the processing function\n",
    "    def process_task(sub_df, start_date, end_date):\n",
    "        return signal_process_wheel(sub_df, start_date, end_date)\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_task)(sub_df, start, end)\n",
    "        for sub_df, start, end in tqdm(tasks, desc=\"Parallel wheel processing\")\n",
    "    )\n",
    "\n",
    "    def chunked_concat(dfs, chunk_size=1000):\n",
    "        chunks = [pd.concat(dfs[i:i+chunk_size], ignore_index=True)\n",
    "                  for i in range(0, len(dfs), chunk_size)]\n",
    "        return pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "    return chunked_concat(results)\n",
    "\n",
    "\n",
    "def decay_weighted_mean(series, window):\n",
    "    half_window = window // 2\n",
    "    full_weights = 1 / (np.abs(np.arange(window) - half_window) + 1)\n",
    "\n",
    "    padded = pd.Series([np.nan] * half_window + list(series) + [np.nan] * half_window)\n",
    "\n",
    "    def weighted_avg(x):\n",
    "        weights = full_weights[:len(x)]  \n",
    "        valid = ~np.isnan(x)\n",
    "        if valid.sum() == 0:\n",
    "            return np.nan\n",
    "        return np.average(x[valid], weights=weights[valid])  \n",
    "\n",
    "    result = padded.rolling(window, center=True, min_periods=1).apply(weighted_avg, raw=True)\n",
    "    return result.iloc[half_window:-half_window].reset_index(drop=True)\n",
    "\n",
    "def compute_score(orig, filtered, complexity=None, weight=1.0):\n",
    "    snr = snr_gain(orig, filtered)\n",
    "    hf_red = hf_var_reduction(orig, filtered)\n",
    "    var_penalty = np.nanvar(filtered) / (np.nanvar(orig) + 1e-12)\n",
    "\n",
    "    base_score = (1.0 * snr) + (0.5 * hf_red) - (2.0 * (1 - var_penalty))\n",
    "\n",
    "    if complexity is not None:\n",
    "        base_score -= weight * complexity\n",
    "\n",
    "    return base_score\n",
    "\n",
    "def signal_process_wheel(df, start_date, end_date):\n",
    "    # column definition\n",
    "    static_cols = ['equipmentnumber', 'truck', 'axle', 'side', 'applieddate']\n",
    "    monthly_cols = ['siteid', 'direction', 'trainspeed']\n",
    "\n",
    "    # if no end date, that means it never updates again so end is the end of march\n",
    "    if pd.isna(end_date):\n",
    "        end_date = pd.to_datetime('2024-11-30')\n",
    "\n",
    "    # making the df with static cols\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    static_values = {col: df.iloc[0][col] for col in static_cols}\n",
    "    base_df = pd.DataFrame(date_range, columns=['traindate'])\n",
    "    for col, val in static_values.items():\n",
    "        base_df[col] = val\n",
    "\n",
    "    # merging the monthly rows from the main set\n",
    "    df_subset = df[['traindate'] + monthly_cols + geometry_features].copy()\n",
    "    df_subset = df_subset.sort_values('traindate', ascending=False)  # keep latest per day\n",
    "    df_subset['traindate'] = pd.to_datetime(df_subset['traindate']).dt.normalize()\n",
    "    df_subset = df_subset.drop_duplicates(subset='traindate')\n",
    "\n",
    "    merged_df = pd.merge(base_df, df_subset, on='traindate', how='left')\n",
    "\n",
    "    # forward-fill back fill monthly rows\n",
    "    #merged_df[monthly_cols] = merged_df[monthly_cols].ffill().bfill()\n",
    "\n",
    "    # go through each feature for FFT + rolling + poly analysis\n",
    "    for feat in geometry_features:\n",
    "        df_feat = merged_df[['traindate', feat]].copy()\n",
    "        df_feat[feat] = df_feat[feat].replace(0, np.nan)\n",
    "        df_feat = df_feat.dropna().sort_values('traindate')\n",
    "\n",
    "        # --- handle trivial cases ---\n",
    "        if len(df_feat) < 3:\n",
    "            merged_df[f'{feat}_original'] = np.nan\n",
    "            for suffix in [\"lowpass\", \"loess\", \"ema\",\"gpr\",\"kalman\",\"l1trend\"]:\n",
    "                merged_df[f'{feat}_{suffix}'] = np.nan\n",
    "                merged_df[f'{feat}_{suffix}_param1'] = np.nan\n",
    "                merged_df[f'{feat}_{suffix}_score'] = np.nan\n",
    "            for suffix in [\"ssa\"]:\n",
    "                merged_df[f'{feat}_{suffix}'] = np.nan\n",
    "                merged_df[f'{feat}_{suffix}_param1'] = np.nan\n",
    "                merged_df[f'{feat}_{suffix}_param2'] = np.nan\n",
    "                merged_df[f'{feat}_{suffix}_score'] = np.nan\n",
    "            continue\n",
    "\n",
    "        # --- data prep ---\n",
    "        x = (df_feat['traindate'] - start_date).dt.days.values\n",
    "        y = df_feat[feat].values\n",
    "        max_days = (end_date - start_date).days\n",
    "\n",
    "        x_uniform = np.linspace(x.min(), max_days, len(x))\n",
    "        y_interp = np.interp(x_uniform, x, y)\n",
    "\n",
    "        x_days_full = (date_range - start_date).days.values\n",
    "        original_daily = np.interp(x_days_full, x, y)\n",
    "\n",
    "        x_min, x_max = x.min(), x.max()\n",
    "\n",
    "        # =======================================================\n",
    "        # FFT LOWPASS\n",
    "        # =======================================================\n",
    "        # --- objective function ---\n",
    "        def objective_fft(cutoff):\n",
    "            n = len(x_uniform)\n",
    "            d = x_uniform[1] - x_uniform[0]\n",
    "            freqs = fftfreq(n, d=d)\n",
    "\n",
    "            Y = fft(y_interp)\n",
    "            Y_low = Y.copy()\n",
    "            Y_low[np.abs(freqs) >= cutoff] = 0\n",
    "            lowpass = np.real(ifft(Y_low))\n",
    "\n",
    "            lowpass_daily = np.interp(x_days_full, x_uniform, lowpass)\n",
    "\n",
    "            # mask outside observed range\n",
    "            lowpass_daily[x_days_full < x_min] = np.nan\n",
    "            lowpass_daily[x_days_full > x_max] = np.nan\n",
    "            orig_masked = original_daily.copy()\n",
    "            orig_masked[x_days_full < x_min] = np.nan\n",
    "            orig_masked[x_days_full > x_max] = np.nan\n",
    "            nyquist = 0.5 * (1.0 / d)\n",
    "            complexity = cutoff/nyquist\n",
    "            score = compute_score(orig_masked, lowpass_daily, complexity)\n",
    "            return -score  # optimizer minimizes\n",
    "\n",
    "        res_fft = minimize_scalar(objective_fft, bounds=(0.0001, 0.1), method=\"bounded\")\n",
    "        best_cutoff_fft = res_fft.x\n",
    "        best_score_fft = -res_fft.fun\n",
    "\n",
    "        n = len(x_uniform)\n",
    "        d = x_uniform[1] - x_uniform[0]\n",
    "        freqs = fftfreq(n, d=d)\n",
    "        Y = fft(y_interp)\n",
    "        Y[np.abs(freqs) >= best_cutoff_fft] = 0\n",
    "        lowpass = np.real(ifft(Y))\n",
    "        best_series_fft = np.interp(x_days_full, x_uniform, lowpass)\n",
    "\n",
    "        best_series_fft[x_days_full < x_min] = np.nan\n",
    "        best_series_fft[x_days_full > x_max] = np.nan\n",
    "        original_daily[x_days_full < x_min] = np.nan\n",
    "        original_daily[x_days_full > x_max] = np.nan\n",
    "\n",
    "        # --- save FFT result ---\n",
    "        merged_df[f'{feat}_original'] = original_daily\n",
    "        merged_df[f'{feat}_lowpass'] = best_series_fft\n",
    "        merged_df[f'{feat}_lowpass_param1'] = best_cutoff_fft\n",
    "        merged_df[f'{feat}_lowpass_score'] = best_score_fft\n",
    "        \n",
    "        # =======================================================\n",
    "        # LOESS (LOWESS)\n",
    "        # =======================================================\n",
    "\n",
    "        # --- objective function ---\n",
    "        def objective_loess(frac):\n",
    "            # frac is the span of data used in each local regression\n",
    "            loess_fit = lowess(y_interp, x_uniform, frac=frac, return_sorted=False)\n",
    "\n",
    "            loess_daily = np.interp(x_days_full, x_uniform, loess_fit)\n",
    "\n",
    "            # mask outside observed range\n",
    "            loess_daily[x_days_full < x_min] = np.nan\n",
    "            loess_daily[x_days_full > x_max] = np.nan\n",
    "            orig_masked = original_daily.copy()\n",
    "            orig_masked[x_days_full < x_min] = np.nan\n",
    "            orig_masked[x_days_full > x_max] = np.nan\n",
    "\n",
    "            # complexity = how much of the dataset each local regression sees\n",
    "            # smaller frac = more flexible (higher complexity), larger = smoother (lower complexity)\n",
    "            complexity = 1.0 - frac  \n",
    "            score = compute_score(orig_masked, loess_daily, complexity)\n",
    "            return -score\n",
    "\n",
    "        # optimize frac in [0.05, 0.8]\n",
    "        res_loess = minimize_scalar(objective_loess, bounds=(0.05, 0.8), method=\"bounded\")\n",
    "        best_frac_loess = res_loess.x\n",
    "        best_score_loess = -res_loess.fun\n",
    "\n",
    "        # recompute best series\n",
    "        loess_fit = lowess(y_interp, x_uniform, frac=best_frac_loess, return_sorted=False)\n",
    "        best_series_loess = np.interp(x_days_full, x_uniform, loess_fit)\n",
    "\n",
    "        best_series_loess[x_days_full < x_min] = np.nan\n",
    "        best_series_loess[x_days_full > x_max] = np.nan\n",
    "\n",
    "        # --- save LOESS result ---\n",
    "        merged_df[f'{feat}_loess'] = best_series_loess\n",
    "        merged_df[f'{feat}_loess_param1'] = best_frac_loess\n",
    "        merged_df[f'{feat}_loess_score'] = best_score_loess\n",
    "\n",
    "        # =======================================================\n",
    "        # Exponential Moving Average (EMA)\n",
    "        # =======================================================\n",
    "\n",
    "        # --- objective function ---\n",
    "        def objective_ema(span):\n",
    "            # EMA smoothing\n",
    "            ema_series = pd.Series(y_interp).ewm(span=span, adjust=False).mean().values\n",
    "\n",
    "            ema_daily = np.interp(x_days_full, x_uniform, ema_series)\n",
    "\n",
    "            # mask outside observed range\n",
    "            ema_daily[x_days_full < x_min] = np.nan\n",
    "            ema_daily[x_days_full > x_max] = np.nan\n",
    "            orig_masked = original_daily.copy()\n",
    "            orig_masked[x_days_full < x_min] = np.nan\n",
    "            orig_masked[x_days_full > x_max] = np.nan\n",
    "\n",
    "            # complexity = smaller span = more reactive (higher complexity)\n",
    "            complexity = 1.0 / span  \n",
    "            score = compute_score(orig_masked, ema_daily, complexity)\n",
    "            return -score\n",
    "\n",
    "        # optimize span (minimum 2, up to say 0.25 * series length to keep long-term trend)\n",
    "        upper_bound = max(5, len(x_uniform) // 4)\n",
    "        res_ema = minimize_scalar(objective_ema, bounds=(2, upper_bound), method=\"bounded\")\n",
    "        best_span_ema = int(res_ema.x)\n",
    "        best_score_ema = -res_ema.fun\n",
    "\n",
    "        # recompute best series\n",
    "        ema_series = pd.Series(y_interp).ewm(span=best_span_ema, adjust=False).mean().values\n",
    "        best_series_ema = np.interp(x_days_full, x_uniform, ema_series)\n",
    "\n",
    "        best_series_ema[x_days_full < x_min] = np.nan\n",
    "        best_series_ema[x_days_full > x_max] = np.nan\n",
    "\n",
    "        # --- save EMA result ---\n",
    "        merged_df[f'{feat}_ema'] = best_series_ema\n",
    "        merged_df[f'{feat}_ema_param1'] = best_span_ema\n",
    "        merged_df[f'{feat}_ema_score'] = best_score_ema\n",
    "        \n",
    "        # =======================================================\n",
    "        # Gaussian Process Regression (RBF Kernel)\n",
    "        # =======================================================\n",
    "        from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "        from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "\n",
    "        # --- objective function ---\n",
    "        def objective_gpr(length_scale):\n",
    "            # Define kernel: RBF + small noise term\n",
    "            kernel = RBF(length_scale=length_scale) + WhiteKernel(noise_level=1e-3)\n",
    "\n",
    "            try:\n",
    "                gp = GaussianProcessRegressor(kernel=kernel, alpha=0.0, normalize_y=True)\n",
    "                gp.fit(x_uniform.reshape(-1, 1), y_interp)\n",
    "\n",
    "                gpr_fit, _ = gp.predict(x_uniform.reshape(-1, 1), return_std=True)\n",
    "            except Exception:\n",
    "                return np.inf\n",
    "\n",
    "            gpr_daily = np.interp(x_days_full, x_uniform, gpr_fit)\n",
    "\n",
    "            # mask outside observed range\n",
    "            gpr_daily[x_days_full < x_min] = np.nan\n",
    "            gpr_daily[x_days_full > x_max] = np.nan\n",
    "            orig_masked = original_daily.copy()\n",
    "            orig_masked[x_days_full < x_min] = np.nan\n",
    "            orig_masked[x_days_full > x_max] = np.nan\n",
    "\n",
    "            # Complexity: shorter length_scale → more wiggly; longer → smoother\n",
    "            complexity = 1.0 / length_scale\n",
    "            score = compute_score(orig_masked, gpr_daily, complexity)\n",
    "            return -score\n",
    "\n",
    "        # Optimize length_scale in a reasonable range (tune based on days scale)\n",
    "        res_gpr = minimize_scalar(objective_gpr, bounds=(5, max(20, len(x_uniform)//2)), method=\"bounded\")\n",
    "        best_ls_gpr = res_gpr.x\n",
    "        best_score_gpr = -res_gpr.fun\n",
    "\n",
    "        # Recompute with best length scale\n",
    "        kernel = RBF(length_scale=best_ls_gpr) + WhiteKernel(noise_level=1e-3)\n",
    "        gp = GaussianProcessRegressor(kernel=kernel, alpha=0.0, normalize_y=True)\n",
    "        gp.fit(x_uniform.reshape(-1, 1), y_interp)\n",
    "        gpr_fit, _ = gp.predict(x_uniform.reshape(-1, 1), return_std=True)\n",
    "        best_series_gpr = np.interp(x_days_full, x_uniform, gpr_fit)\n",
    "\n",
    "        best_series_gpr[x_days_full < x_min] = np.nan\n",
    "        best_series_gpr[x_days_full > x_max] = np.nan\n",
    "\n",
    "        # --- save GPR result ---\n",
    "        merged_df[f'{feat}_gpr'] = best_series_gpr\n",
    "        merged_df[f'{feat}_gpr_param1'] = best_ls_gpr\n",
    "        merged_df[f'{feat}_gpr_score'] = best_score_gpr\n",
    "\n",
    "\n",
    "        # =======================================================\n",
    "        # KALMAN FILTER (Local Level Model, State Space)\n",
    "        # =======================================================\n",
    "        from statsmodels.tsa.statespace.structural import UnobservedComponents\n",
    "        \n",
    "        def run_kf(y_interp, x_uniform, x_days_full, x_min, x_max, original_daily, proc_var):\n",
    "            # Local level state-space model\n",
    "            mod = UnobservedComponents(y_interp, level=\"local level\")\n",
    "\n",
    "            # Set process and measurement noise manually\n",
    "            mod.ssm['state_cov', 0, 0] = proc_var\n",
    "            mod.ssm['obs_cov', 0, 0] = np.nanvar(y_interp) * 0.1  # fix obs variance\n",
    "\n",
    "            # Run the filter/smoother directly\n",
    "            kf_res = mod.ssm.smooth()\n",
    "            kf_fit = kf_res.smoothed_state[0]\n",
    "\n",
    "            # Interpolate to daily\n",
    "            kf_daily = np.interp(x_days_full, x_uniform, kf_fit)\n",
    "            kf_daily[x_days_full < x_min] = np.nan\n",
    "            kf_daily[x_days_full > x_max] = np.nan\n",
    "\n",
    "            # Mask original\n",
    "            orig_masked = original_daily.copy()\n",
    "            orig_masked[x_days_full < x_min] = np.nan\n",
    "            orig_masked[x_days_full > x_max] = np.nan\n",
    "\n",
    "            return kf_daily, orig_masked\n",
    "\n",
    "        # --- objective function ---\n",
    "        def objective_kf(proc_var):\n",
    "            try:\n",
    "                kf_daily, orig_masked = run_kf(\n",
    "                    y_interp, x_uniform, x_days_full, x_min, x_max, original_daily, proc_var\n",
    "                )\n",
    "                complexity = np.clip(proc_var, 1e-6, 1.0)\n",
    "                score = compute_score(orig_masked, kf_daily, complexity)\n",
    "                return -score\n",
    "            except Exception:\n",
    "                return np.inf\n",
    "\n",
    "        # Grid search in log-space\n",
    "        search_grid = np.logspace(-4, 0, 20)\n",
    "        scores = []\n",
    "        for pv in search_grid:\n",
    "            s = objective_kf(pv)\n",
    "            scores.append((pv, -s))\n",
    "\n",
    "        best_proc_var, best_score_kf = max(scores, key=lambda x: x[1])\n",
    "\n",
    "        # Recompute best KF fit\n",
    "        best_series_kf, _ = run_kf(y_interp, x_uniform, x_days_full, x_min, x_max, original_daily, best_proc_var)\n",
    "\n",
    "        # --- save KF result ---\n",
    "        merged_df[f'{feat}_kalman'] = best_series_kf\n",
    "        merged_df[f'{feat}_kalman_param1'] = best_proc_var\n",
    "        merged_df[f'{feat}_kalman_score'] = best_score_kf\n",
    "\n",
    "        # =======================================================\n",
    "        # ℓ1 Trend Filtering (Total Variation Denoising)\n",
    "        # =======================================================\n",
    "        def run_l1_trend(y_interp, x_uniform, x_days_full, x_min, x_max, original_daily, lam):\n",
    "            n = len(y_interp)\n",
    "            tau = cp.Variable(n)\n",
    "            if n < 3:\n",
    "                # Not enough points for 2nd-order differences\n",
    "                return np.full_like(x_days_full, np.nan, dtype=float), original_daily\n",
    "            y = np.asarray(y_interp, dtype=float).flatten()\n",
    "            # Use cvxpy's diff to avoid any shape bugs\n",
    "            objective = cp.Minimize(0.5 * cp.sum_squares(tau - y) + lam * cp.norm1(cp.diff(tau, 2)))\n",
    "            prob = cp.Problem(objective)\n",
    "            prob.solve(solver=cp.OSQP, verbose=False)  # ECOS or SCS also work\n",
    "\n",
    "            if tau.value is None:\n",
    "                raise RuntimeError(\"ℓ1 trend filtering failed\")\n",
    "\n",
    "            tau_fit = tau.value\n",
    "            l1_daily = np.interp(x_days_full, x_uniform, tau_fit)\n",
    "            l1_daily[x_days_full < x_min] = np.nan\n",
    "            l1_daily[x_days_full > x_max] = np.nan\n",
    "\n",
    "            orig_masked = original_daily.copy()\n",
    "            orig_masked[x_days_full < x_min] = np.nan\n",
    "            orig_masked[x_days_full > x_max] = np.nan\n",
    "            return l1_daily, orig_masked\n",
    "\n",
    "        # --- objective function ---\n",
    "        def objective_l1(lam):\n",
    "            try:\n",
    "                l1_daily, orig_masked = run_l1_trend(\n",
    "                    y_interp, x_uniform, x_days_full, x_min, x_max, original_daily, lam\n",
    "                )\n",
    "                # smaller λ → more flexible (higher complexity); larger → smoother (lower complexity)\n",
    "                complexity = 1.0 / (1.0 + lam)\n",
    "                score = compute_score(orig_masked, l1_daily, complexity)\n",
    "                return -score\n",
    "            except Exception:\n",
    "                return np.inf\n",
    "\n",
    "\n",
    "        # Grid search in log-space\n",
    "        search_grid = np.logspace(-3, 2, 20)  # λ candidates\n",
    "        scores = []\n",
    "        for lam in search_grid:\n",
    "            s = objective_l1(lam)\n",
    "            scores.append((lam, -s))\n",
    "\n",
    "        best_lam, best_score_l1 = max(scores, key=lambda x: x[1])\n",
    "\n",
    "        # Recompute best L1 trend filtering fit\n",
    "        best_series_l1, _ = run_l1_trend(\n",
    "            y_interp, x_uniform, x_days_full, x_min, x_max, original_daily, best_lam\n",
    "        )\n",
    "\n",
    "        # --- save L1 result ---\n",
    "        merged_df[f'{feat}_l1trend'] = best_series_l1\n",
    "        merged_df[f'{feat}_l1trend_param1'] = best_lam\n",
    "        merged_df[f'{feat}_l1trend_score'] = best_score_l1\n",
    "\n",
    "        # =======================================================\n",
    "        # Singular Spectrum Analysis (SSA, low-rank modes)\n",
    "        # =======================================================\n",
    "        def _ssa_trajectory(y, L):\n",
    "            \"\"\"Build L x K Hankel trajectory matrix.\"\"\"\n",
    "            n = len(y)\n",
    "            L = int(L)\n",
    "            K = n - L + 1\n",
    "            X = np.column_stack([y[i:i+L] for i in range(K)])  # shape (L, K)\n",
    "            return X  # Hankel/trajectory matrix\n",
    "\n",
    "        def _ssa_hankel_averaging(X):\n",
    "            \"\"\"Diagonal averaging (Hankelization) back to 1D series of length n = L+K-1.\"\"\"\n",
    "            L, K = X.shape\n",
    "            n = L + K - 1\n",
    "            y_rec = np.zeros(n, dtype=float)\n",
    "            counts = np.zeros(n, dtype=float)\n",
    "\n",
    "            # sum over anti-diagonals: indices (i, j) with i+j = k\n",
    "            for i in range(L):\n",
    "                for j in range(K):\n",
    "                    y_rec[i + j] += X[i, j]\n",
    "                    counts[i + j] += 1.0\n",
    "            y_rec /= np.maximum(counts, 1.0)\n",
    "            return y_rec\n",
    "\n",
    "        def run_ssa(y_interp, x_uniform, x_days_full, x_min, x_max, original_daily,\n",
    "                    L_candidates=None, r_max=10):\n",
    "            y = np.asarray(y_interp, dtype=float).flatten()\n",
    "            n = y.size\n",
    "            if n < 3:\n",
    "                # Too short for meaningful SSA\n",
    "                return (np.full_like(x_days_full, np.nan, dtype=float),\n",
    "                        {\"L\": np.nan, \"r\": np.nan, \"score\": -np.inf})\n",
    "\n",
    "            # Default window candidates (safe & small set)\n",
    "            if L_candidates is None:\n",
    "                L_candidates = sorted(set([\n",
    "                    max(2, min(n-1, n // 2)),\n",
    "                    max(2, min(n-1, n // 3)),\n",
    "                    max(2, min(n-1, n // 4))\n",
    "                ]))\n",
    "\n",
    "            best = {\"score\": -np.inf, \"L\": None, \"r\": None, \"daily\": None}\n",
    "\n",
    "            for L in L_candidates:\n",
    "                K = n - L + 1\n",
    "                if K < 2:  # need at least 2 columns\n",
    "                    continue\n",
    "\n",
    "                # Build trajectory and SVD once per L\n",
    "                X = _ssa_trajectory(y, L)           # (L, K)\n",
    "                U, s, Vt = np.linalg.svd(X, full_matrices=False)  # U:(L,r), s:(r,), Vt:(r,K)\n",
    "                r_cap = int(min(r_max, s.size))\n",
    "\n",
    "                # Incrementally add components to avoid recomputing\n",
    "                Xr = np.zeros_like(X)\n",
    "                for r in range(1, r_cap + 1):\n",
    "                    # rank-r reconstruction\n",
    "                    ur = U[:, r-1]\n",
    "                    sr = s[r-1]\n",
    "                    vr = Vt[r-1, :]\n",
    "                    Xr += sr * np.outer(ur, vr)\n",
    "\n",
    "                    y_rec = _ssa_hankel_averaging(Xr)  # length n = L+K-1 == n\n",
    "                    # Interp to daily grid\n",
    "                    ssa_daily = np.interp(x_days_full, x_uniform, y_rec)\n",
    "\n",
    "                    # mask outside observed span\n",
    "                    ssa_daily[x_days_full < x_min] = np.nan\n",
    "                    ssa_daily[x_days_full > x_max] = np.nan\n",
    "\n",
    "                    # mask original to same span\n",
    "                    orig_masked = original_daily.copy()\n",
    "                    orig_masked[x_days_full < x_min] = np.nan\n",
    "                    orig_masked[x_days_full > x_max] = np.nan\n",
    "\n",
    "                    # complexity: relative rank\n",
    "                    complexity = r / float(min(L, K))\n",
    "                    score = compute_score(orig_masked, ssa_daily, complexity)\n",
    "\n",
    "                    if score > best[\"score\"]:\n",
    "                        best.update({\"score\": score, \"L\": L, \"r\": r, \"daily\": ssa_daily})\n",
    "\n",
    "            if best[\"daily\"] is None:\n",
    "                # fallback if all candidates invalid\n",
    "                return (np.full_like(x_days_full, np.nan, dtype=float),\n",
    "                        {\"L\": np.nan, \"r\": np.nan, \"score\": -np.inf})\n",
    "\n",
    "            return best[\"daily\"], {\"L\": best[\"L\"], \"r\": best[\"r\"], \"score\": best[\"score\"]}\n",
    "\n",
    "        # --- run SSA & save ---\n",
    "        ssa_daily, ssa_meta = run_ssa(\n",
    "            y_interp, x_uniform, x_days_full, x_min, x_max, original_daily,\n",
    "            L_candidates=None,  # or pass a list like [30, 45, 60]\n",
    "            r_max=10\n",
    "        )\n",
    "\n",
    "        merged_df[f'{feat}_ssa'] = ssa_daily\n",
    "        merged_df[f'{feat}_ssa_param1'] = ssa_meta[\"L\"]\n",
    "        merged_df[f'{feat}_ssa_param2'] = ssa_meta[\"r\"]\n",
    "        merged_df[f'{feat}_ssa_score'] = ssa_meta[\"score\"]\n",
    "\n",
    "    # dropping raw cols as we now have _original cols\n",
    "    #merged_df = merged_df.drop(columns=geometry_features)\n",
    "    merged_df['days'] = (pd.to_datetime(merged_df['traindate']) - pd.to_datetime(start_date)).dt.days\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de356e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_wpd = interpolate_daily_wheels(df_wpd, -1)\n",
    "#full_wpd.to_feather('wpd_trend_null.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a77aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_signal_metrics(full_wpd, geometry_features, type, param1, use_param2=False):\n",
    "    results = []\n",
    "\n",
    "    def lag1_autocorr(x):\n",
    "        return np.corrcoef(x[:-1], x[1:])[0,1] if len(x) > 1 else np.nan\n",
    "\n",
    "    for feat in geometry_features:\n",
    "        orig = full_wpd[f\"{feat}_original\"].values\n",
    "        lowp = full_wpd[f\"{feat}_{type}\"].values\n",
    "\n",
    "        # mask invalid\n",
    "        mask = ~np.isnan(orig) & ~np.isnan(lowp)\n",
    "        if mask.sum() < 3:\n",
    "            snr = hf_red = rmse = var_ratio = acf_diff = np.nan\n",
    "        else:\n",
    "            snr = snr_gain(orig[mask], lowp[mask])\n",
    "            hf_red = hf_var_reduction(orig[mask], lowp[mask])\n",
    "            rmse = np.sqrt(np.mean((orig[mask] - lowp[mask])**2))\n",
    "            var_ratio = np.var(lowp[mask]) / np.var(orig[mask]) if np.var(orig[mask]) > 0 else np.nan\n",
    "            acf_orig = lag1_autocorr(orig[mask])\n",
    "            acf_lowp = lag1_autocorr(lowp[mask])\n",
    "            acf_diff = abs(acf_orig - acf_lowp)\n",
    "\n",
    "        # param1 always present\n",
    "        param1_val = full_wpd[f\"{feat}_{type}_param1\"].dropna().mean()\n",
    "\n",
    "        # param2 only if flagged (ssa case)\n",
    "        if use_param2:\n",
    "            param2_val = full_wpd[f\"{feat}_{type}_param2\"].dropna().mean()\n",
    "            results.append({\n",
    "                \"feature\": feat,\n",
    "                \"param1\": param1_val,\n",
    "                \"param2\": param2_val,\n",
    "                \"snr_gain\": snr,\n",
    "                \"hf_var_reduction\": hf_red,\n",
    "                \"rmse\": rmse,\n",
    "                \"var_ratio\": var_ratio,\n",
    "                \"acf_diff\": acf_diff\n",
    "            })\n",
    "        else:\n",
    "            results.append({\n",
    "                \"feature\": feat,\n",
    "                \"param1\": param1_val,\n",
    "                \"snr_gain\": snr,\n",
    "                \"hf_var_reduction\": hf_red,\n",
    "                \"rmse\": rmse,\n",
    "                \"var_ratio\": var_ratio,\n",
    "                \"acf_diff\": acf_diff\n",
    "            })\n",
    "\n",
    "\n",
    "    return pd.DataFrame(results).set_index(\"feature\")\n",
    "\n",
    "for type in [\"lowpass\", \"loess\", \"ema\", \"gpr\", \"kalman\", \"l1trend\"]:\n",
    "    summary = compute_signal_metrics(full_wpd, geometry_features, type, \"param1\")\n",
    "    display(summary)   # pretty df display\n",
    "\n",
    "summary = compute_signal_metrics(full_wpd, geometry_features, \"ssa\", \"param1\", use_param2=True)\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d74a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "temp = full_wpd[(full_wpd['equipmentnumber'] == 1) & (full_wpd['axle'] == 1) & (full_wpd['side'] == 'L')]['applieddate'].unique()\n",
    "temp = full_wpd[(full_wpd['equipmentnumber'] == 1) & (full_wpd['axle'] == 1) & (full_wpd['side'] == 'L') & (full_wpd['applieddate'] == temp[1])]\n",
    "# make sure we have days (if not already in full_wpd)\n",
    "if \"days\" not in temp.columns:\n",
    "    temp[\"days\"] = (temp[\"traindate\"] - temp[\"traindate\"].min()).dt.days\n",
    "\n",
    "signal = \"kalman\"\n",
    "# find all features that have a lowpass\n",
    "features = [c.replace(f\"_{signal}\", \"\") for c in temp.columns if c.endswith(f\"_{signal}\")]\n",
    "\n",
    "for feat in features:\n",
    "    raw_col   = f\"{feat}\"\n",
    "    signal_col    = f\"{feat}_{signal}\"\n",
    "\n",
    "    if raw_col not in temp or signal_col not in temp:\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(temp[\"days\"], temp[raw_col], label=\"Raw\", alpha=0.3)\n",
    "    plt.plot(temp[\"days\"], temp[signal_col], color='orange', label=signal.capitalize())\n",
    "\n",
    "    plt.title(f\"{feat} - Raw vs {signal.capitalize()}\")\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e76527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
