{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ca2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.fft import fft, ifft, fftfreq\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import MonthBegin\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76a37de",
   "metadata": {},
   "source": [
    "## Hard-Coded Variables \n",
    "- uses domain knowledge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hardcoded information\n",
    "expected_groups = {\n",
    "    ('A', 7),\n",
    "    ('A', 8),\n",
    "    ('B', 1),\n",
    "    ('B', 2),\n",
    "    ('C', 3),\n",
    "    ('C', 4),\n",
    "    ('D', 5),\n",
    "    ('D', 6),\n",
    "}\n",
    "\n",
    "group_1 = {1,2,7,8}\n",
    "group_2 = {3,4,5,6}\n",
    "group_1_feature_bounds = {\n",
    "    \"wheeldiameter\": (29.0, 41),\n",
    "    \"wheelwidth\": (5.50, 5.9),\n",
    "    \"flangeheight\": (0.90, 1.55),\n",
    "    \"flangethickness\": (0.85, 1.75),\n",
    "    \"flangeslope\": (0.10, 0.60),\n",
    "    \"treadhollow\": (0.01, 5.3),\n",
    "    \"rimthickness\": (0.75, 1.85),\n",
    "    \"backtobackgauge\": (52.0, 53.0),\n",
    "}\n",
    "group_2_feature_bounds = {\n",
    "    \"wheeldiameter\": (29.0, 46.0),\n",
    "    \"wheelwidth\": (5.50, 5.87),\n",
    "    \"flangeheight\": (0.90, 1.55),\n",
    "    \"flangethickness\": (0.85, 1.75),\n",
    "    \"flangeslope\": (0.10, 0.60),\n",
    "    \"treadhollow\": (0.01, 5.1),\n",
    "    \"rimthickness\": (0.80, 2.1),\n",
    "    \"backtobackgauge\": (52.0, 53.0),\n",
    "}\n",
    "\n",
    "group_2_severity_thresholds = {'backtobackgauge': [(52.0, -5), (52.0, -4), (52.0, -3), (52.0, -2), (52.0, -1),\n",
    "                     (53.0, 1), (53.0, 2), (53.0, 3), (53.0, 4), (53.0, 5)],\n",
    " 'flangeheight': [(1.009, -5), (1.027, -4), (1.036, -3), (1.041, -2),\n",
    "                  (1.057, -1), (1.467, 1), (1.487, 2), (1.492, 3), (1.5, 4),\n",
    "                  (1.508, 5)],\n",
    " 'flangeslope': [(0.119, -5), (0.122, -4), (0.124, -3), (0.126, -2),\n",
    "                 (0.132, -1), (0.505, 1), (0.528, 2), (0.535, 3), (0.544, 4),\n",
    "                 (0.556, 5)],\n",
    " 'flangethickness': [(0.878, -5), (0.897, -4), (0.911, -3), (0.924, -2),\n",
    "                     (0.969, -1), (1.544, 1), (1.572, 2), (1.582, 3),\n",
    "                     (1.597, 4), (1.62, 5)],\n",
    " 'rimthickness': [(0.852, -5), (0.868, -4), (0.88, -3), (0.889, -2), (0.92, -1),\n",
    "                  (1.983, 1), (2.012, 2), (2.021, 3), (2.032, 4), (2.05, 5)],\n",
    " 'treadhollow': [(0.01, -5), (0.01, -4), (0.01, -3), (0.01, -2), (0.01, -1),\n",
    "                 (4.029, 1), (4.369, 2), (4.473, 3), (4.601, 4), (4.769, 5)],\n",
    " 'wheeldiameter': [(29.0, -5), (29.134, -4), (30.0, -3), (30.63, -2),\n",
    "                   (32.756, -1), (43.622, 1), (44.331, 2), (44.488, 3),\n",
    "                   (44.724, 4), (45.118, 5)],\n",
    " 'wheelwidth': [(5.516, -5), (5.526, -4), (5.533, -3), (5.539, -2), (5.554, -1),\n",
    "                (5.812, 1), (5.831, 2), (5.837, 3), (5.843, 4), (5.852, 5)]}\n",
    "\n",
    "group_1_severity_thresholds = {\n",
    "    'backtobackgauge': [(52.0, -5), (52.0, -4), (52.0, -3), (52.0, -2), (52.0, -1),\n",
    "                     (53.0, 1), (53.0, 2), (53.0, 3), (53.0, 4), (53.0, 5)],\n",
    " 'flangeheight': [(1.03, -5), (1.044, -4), (1.051, -3), (1.058, -2),\n",
    "                  (1.072, -1), (1.5, 1), (1.515, 2), (1.52, 3), (1.525, 4),\n",
    "                  (1.533, 5)],\n",
    " 'flangeslope': [(0.124, -5), (0.128, -4), (0.13, -3), (0.132, -2), (0.139, -1),\n",
    "                 (0.52, 1), (0.543, 2), (0.55, 3), (0.559, 4), (0.572, 5)],\n",
    " 'flangethickness': [(0.879, -5), (0.895, -4), (0.909, -3), (0.92, -2),\n",
    "                     (0.96, -1), (1.575, 1), (1.604, 2), (1.615, 3), (1.63, 4),\n",
    "                     (1.653, 5)],\n",
    " 'rimthickness': [(0.778, -5), (0.796, -4), (0.81, -3), (0.822, -2),\n",
    "                  (0.866, -1), (1.694, 1), (1.726, 2), (1.736, 3), (1.749, 4),\n",
    "                  (1.768, 5)],\n",
    " 'treadhollow': [(0.01, -5), (0.01, -4), (0.01, -3), (0.01, -2), (0.01, -1),\n",
    "                 (4.196, 1), (4.53, 2), (4.636, 3), (4.757, 4), (4.938, 5)],\n",
    " 'wheeldiameter': [(30.394, -5), (30.709, -4), (30.945, -3), (31.102, -2),\n",
    "                   (31.575, -1), (38.976, 1), (39.528, 2), (39.764, 3),\n",
    "                   (40.0, 4), (40.236, 5)],\n",
    " 'wheelwidth': [(5.527, -5), (5.537, -4), (5.543, -3), (5.548, -2), (5.563, -1),\n",
    "                (5.812, 1), (5.833, 2), (5.839, 3), (5.845, 4), (5.853, 5)]\n",
    "}\n",
    "\n",
    "geometry_features = ['flangeheight', 'rimthickness', 'wheeldiameter', 'wheelwidth','flangethickness','flangeslope', 'backtobackgauge','treadhollow', 'flangeangle']  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a746c4dc",
   "metadata": {},
   "source": [
    "## Initializations\n",
    "- loading datasets\n",
    "- filtering invalid values and axles\n",
    "- merging failure dfs into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339acb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def load_data(path_prefix: str = '../Datasets') -> dict:\n",
    "    \"\"\"\n",
    "    Load all required datasets and fill missing failure reasons.\n",
    "    Returns a dict of DataFrames.\n",
    "    \"\"\"\n",
    "    files = {\n",
    "        'failure': 'FailureTable0723.csv',\n",
    "        'equipment': 'equipment_data_masked.csv',\n",
    "        'mileage': 'Mileage0723.csv',\n",
    "        'wpd': 'Wpd0723.csv',\n",
    "    }\n",
    "    dfs = {name: pd.read_csv(f\"{path_prefix}/{fname}\", engine='pyarrow') for name, fname in files.items()}\n",
    "    dfs['failure']['failurereason'] = dfs['failure']['failurereason'].fillna('not failed')\n",
    "    dfs['wpd']= dfs['wpd'][dfs['wpd']['traindate'] < '2024-12-01']\n",
    "    return dfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa71425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with failurereason, and applieddate and mileage data\n",
    "def fast_filter(df, name,valid_axles, feature_bounds):\n",
    "    before = df.shape\n",
    "    valid_trucks = {(truck) for (truck, axle) in expected_groups if axle in valid_axles}\n",
    "    valid_pairs = {(truck, axle) for (truck, axle) in expected_groups if axle in valid_axles}\n",
    "\n",
    "    if 'axle' not in df.columns:\n",
    "        mask = df['truck'].isin(valid_trucks)\n",
    "    else:\n",
    "        mask = [pair in valid_pairs for pair in zip(df['truck'], df['axle'])]\n",
    "\n",
    "    df_filtered = df[mask].copy()\n",
    "    df_filtered[geometry_features + ['trainspeed']] = df_filtered[geometry_features + ['trainspeed']].replace(0, np.nan)\n",
    "    for feature, (min_val, max_val) in feature_bounds.items():\n",
    "        if feature in df_filtered.columns:\n",
    "            too_low = df_filtered[feature] < (0.8 * min_val)\n",
    "            too_high = df_filtered[feature] > (1.2 * max_val)\n",
    "\n",
    "            df_filtered.loc[too_low | too_high, feature] = np.nan\n",
    "\n",
    "            df_filtered.loc[(df_filtered[feature] >= (0.8 * min_val)) & (df_filtered[feature] < min_val), feature] = min_val\n",
    "            df_filtered.loc[(df_filtered[feature] <= (1.2 * max_val)) & (df_filtered[feature] > max_val), feature] = max_val\n",
    "\n",
    "    after = df_filtered.shape\n",
    "    print(f\"{name}: before = {before}, after = {after}\")\n",
    "    return df_filtered.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def fast_filter_by_group(df, name):\n",
    "    dfs = df.copy()\n",
    "    print(f\"{name}: pre-filter shape: {dfs.shape}\")\n",
    "    df_group_1 = fast_filter(dfs, f\"{name} (group1)\", group_1, group_1_feature_bounds)\n",
    "    df_group_2 = fast_filter(dfs, f\"{name} (group2)\", group_2, group_2_feature_bounds)\n",
    "    df_combined = pd.concat([df_group_1, df_group_2], ignore_index=True)\n",
    "    print(f\"{name}: combined shape = {df_combined.shape}\")\n",
    "\n",
    "    return df_combined\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbafa1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "dfs = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wpd = fast_filter_by_group(dfs['wpd'], 'wpd')\n",
    "df_failure = dfs['failure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f27e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wpd = df_wpd[df_wpd['equipmentnumber'] <= 5]\n",
    "df_failure = df_failure[df_failure['equipmentnumber'] <= 5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5cef4",
   "metadata": {},
   "source": [
    "## Adding applieddates to wpd records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ef8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wpd['recordmonth'] = df_wpd['traindate'].values.astype('datetime64[M]')\n",
    "df_wpd['recordmonth_next'] = df_wpd['recordmonth'] + pd.DateOffset(months=1)\n",
    "\n",
    "# getting rid of duplicates due to vendornumbersuppliercode \n",
    "df_failure_temp = df_failure.sort_values('applieddate').drop_duplicates(['equipmentnumber', 'truck', 'axle', 'side', 'recordmonth'])\n",
    "df_failure_temp['recordmonth'] = pd.to_datetime(df_failure_temp['recordmonth']).values.astype('datetime64[M]')\n",
    "\n",
    "# merging current month's applieddate\n",
    "merge_keys = ['equipmentnumber', 'truck', 'axle', 'side', 'recordmonth']\n",
    "df_wpd = df_wpd.merge(\n",
    "    df_failure_temp[merge_keys + ['applieddate']],\n",
    "    on=merge_keys,\n",
    "    how='left'\n",
    ")\n",
    "df_wpd.rename(columns={'applieddate': 'applieddate_initial'}, inplace=True)\n",
    "\n",
    "# merging next month's applieddate \n",
    "merge_keys_next = ['equipmentnumber', 'truck', 'axle', 'side', 'recordmonth_next']\n",
    "df_wpd = df_wpd.merge(\n",
    "    df_failure_temp.rename(columns={'recordmonth': 'recordmonth_next', 'applieddate': 'applieddate_next'})[\n",
    "        merge_keys_next + ['applieddate_next']\n",
    "    ],\n",
    "    on=merge_keys_next,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# if next months applieddate is before my traindate, I am apart of next month's wheel\n",
    "df_wpd['applieddate'] = np.where(\n",
    "    df_wpd['traindate'] >= df_wpd['applieddate_next'],\n",
    "    df_wpd['applieddate_next'],\n",
    "    df_wpd['applieddate_initial']\n",
    ")\n",
    "\n",
    "df_wpd = df_wpd.drop(columns=['recordmonth_next','applieddate_next','applieddate_initial'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d766e6",
   "metadata": {},
   "source": [
    "## Make DF for each wheel\n",
    "- starts at the applieddate, ends at the day before the next applieddate\n",
    "- apply FFT, decay-weighted rolling mean, and a combination of the 2 as a polynomial trend to the DF \n",
    "- more details in the notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c831c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.fft import fft, ifft, fftfreq\n",
    "from scipy.optimize import minimize_scalar\n",
    "import pywt\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import welch\n",
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "from PyEMD import CEEMDAN\n",
    "\n",
    "def eval_peak_metrics(orig, recon, peaks_true=None, height=None, distance=None):\n",
    "    \"\"\"\n",
    "    Compute anomaly-preserving metrics: peak recall & attenuation ratio.\n",
    "    \"\"\"\n",
    "    if peaks_true is None:\n",
    "        peaks_true, _ = find_peaks(orig, height=height, distance=distance)\n",
    "    peaks_recon, props = find_peaks(recon, height=height, distance=distance)\n",
    "\n",
    "    if len(peaks_true) == 0:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    tol = 2  # indices within ±2 accepted\n",
    "    matched = 0\n",
    "    attn_ratios = []\n",
    "    for pt in peaks_true:\n",
    "        close = np.where(np.abs(peaks_recon - pt) <= tol)[0]\n",
    "        if len(close) > 0:\n",
    "            matched += 1\n",
    "            attn_ratios.append(\n",
    "                np.mean(recon[peaks_recon[close]]) / (orig[pt] + 1e-12)\n",
    "            )\n",
    "\n",
    "    recall = matched / len(peaks_true)\n",
    "    attenuation = np.mean(attn_ratios) if attn_ratios else np.nan\n",
    "    return recall, attenuation\n",
    "\n",
    "\n",
    "def spectral_energy_retention(orig, recon, fs=1.0, band=(1/14, 1/1)):\n",
    "    \"\"\"\n",
    "    Fraction of spectral energy retained in anomaly-relevant band.\n",
    "    band is in cycles/day (e.g. 1/14 to 1/1 ~ 1-14 day periods).\n",
    "    \"\"\"\n",
    "    f_orig, Pxx_orig = welch(orig, fs=fs, nperseg=min(256, len(orig)))\n",
    "    f_recon, Pxx_recon = welch(recon, fs=fs, nperseg=min(256, len(recon)))\n",
    "\n",
    "    band_mask_orig = (f_orig >= band[0]) & (f_orig <= band[1])\n",
    "    band_mask_recon = (f_recon >= band[0]) & (f_recon <= band[1])\n",
    "\n",
    "    energy_orig = np.trapz(Pxx_orig[band_mask_orig], f_orig[band_mask_orig])\n",
    "    energy_recon = np.trapz(Pxx_recon[band_mask_recon], f_recon[band_mask_recon])\n",
    "\n",
    "    return energy_recon / (energy_orig + 1e-12)\n",
    "\n",
    "\n",
    "def false_peak_rate(orig, recon, height=None, distance=None):\n",
    "    \"\"\"\n",
    "    Fraction of peaks in recon that are spurious (not in orig).\n",
    "    \"\"\"\n",
    "    peaks_orig, _ = find_peaks(orig, height=height, distance=distance)\n",
    "    peaks_recon, _ = find_peaks(recon, height=height, distance=distance)\n",
    "\n",
    "    tol = 2\n",
    "    false_peaks = 0\n",
    "    for pr in peaks_recon:\n",
    "        if len(peaks_orig) == 0 or np.min(np.abs(peaks_orig - pr)) > tol:\n",
    "            false_peaks += 1\n",
    "\n",
    "    return false_peaks / (len(recon) + 1e-12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e5cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wpd['traindate'] = pd.to_datetime(df_wpd['traindate'])\n",
    "df_wpd['applieddate'] = pd.to_datetime(df_wpd['applieddate'])\n",
    "df_wpd = df_wpd.sort_values(by=['equipmentnumber', 'truck', 'axle', 'side', 'applieddate', 'traindate'])\n",
    "\n",
    "def interpolate_daily_wheels(df, n_jobs=-1):\n",
    "    group_cols = ['equipmentnumber', 'truck', 'axle', 'side']\n",
    "    grouped = df.groupby(group_cols)\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    # Prepare tasks: each task = (sub_df, start_date, end_date)\n",
    "    for group_keys, group_df in grouped:\n",
    "        subgroups = group_df.groupby('applieddate')\n",
    "        sub_keys_sorted = sorted(subgroups.groups.keys())\n",
    "\n",
    "        for i, adate in enumerate(sub_keys_sorted):\n",
    "            sub_df = subgroups.get_group(adate).copy()\n",
    "            start_date = sub_df['applieddate'].min()\n",
    "            if start_date < pd.to_datetime('2020-01-01'):\n",
    "                start_date = pd.to_datetime('2020-01-01')\n",
    "            if i + 1 < len(sub_keys_sorted):\n",
    "                next_applieddate = sub_keys_sorted[i + 1]\n",
    "                end_date = pd.to_datetime(next_applieddate) - pd.Timedelta(days=1)\n",
    "            else:\n",
    "                end_date = pd.NaT\n",
    "\n",
    "            tasks.append((sub_df, start_date, end_date))\n",
    "\n",
    "    # Wrap the processing function\n",
    "    def process_task(sub_df, start_date, end_date):\n",
    "        return signal_process_wheel(sub_df, start_date, end_date)\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_task)(sub_df, start, end)\n",
    "        for sub_df, start, end in tqdm(tasks, desc=\"Parallel wheel processing\")\n",
    "    )\n",
    "\n",
    "    def chunked_concat(dfs, chunk_size=1000):\n",
    "        chunks = [pd.concat(dfs[i:i+chunk_size], ignore_index=True)\n",
    "                  for i in range(0, len(dfs), chunk_size)]\n",
    "        return pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "    return chunked_concat(results)\n",
    "\n",
    "\n",
    "def signal_process_wheel(df, start_date, end_date):\n",
    "    # column definition\n",
    "    static_cols = ['equipmentnumber', 'truck', 'axle', 'side', 'applieddate']\n",
    "    monthly_cols = ['siteid', 'direction', 'trainspeed']\n",
    "\n",
    "    # if no end date, that means it never updates again so end is the end of march\n",
    "    if pd.isna(end_date):\n",
    "        end_date = pd.to_datetime('2024-11-30')\n",
    "\n",
    "    # making the df with static cols\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    static_values = {col: df.iloc[0][col] for col in static_cols}\n",
    "    base_df = pd.DataFrame(date_range, columns=['traindate'])\n",
    "    for col, val in static_values.items():\n",
    "        base_df[col] = val\n",
    "\n",
    "    # merging the monthly rows from the main set\n",
    "    df_subset = df[['traindate'] + monthly_cols + geometry_features].copy()\n",
    "    df_subset = df_subset.sort_values('traindate', ascending=False)  # keep latest per day\n",
    "    df_subset['traindate'] = pd.to_datetime(df_subset['traindate']).dt.normalize()\n",
    "    df_subset = df_subset.drop_duplicates(subset='traindate')\n",
    "\n",
    "    merged_df = pd.merge(base_df, df_subset, on='traindate', how='left')\n",
    "\n",
    "    # forward-fill back fill monthly rows\n",
    "    #merged_df[monthly_cols] = merged_df[monthly_cols].ffill().bfill()\n",
    "\n",
    "    # go through each feature for FFT + rolling + poly analysis\n",
    "    for feat in geometry_features:\n",
    "        df_feat = merged_df[['traindate', feat]].copy()\n",
    "        df_feat[feat] = df_feat[feat].replace(0, np.nan)\n",
    "        df_feat = df_feat.dropna().sort_values('traindate')\n",
    "\n",
    "        # --- handle trivial cases ---\n",
    "        if len(df_feat) < 3:\n",
    "            merged_df[f'{feat}_original'] = np.nan\n",
    "            for suffix in [\"lowpass\", \"loess\", \"ema\",\"gpr\",\"kalman\",\"l1trend\"]:\n",
    "                merged_df[f'{feat}_{suffix}'] = np.nan\n",
    "                merged_df[f'{feat}_{suffix}_param1'] = np.nan\n",
    "                merged_df[f'{feat}_{suffix}_score'] = np.nan\n",
    "            for suffix in [\"ssa\"]:\n",
    "                merged_df[f'{feat}_{suffix}'] = np.nan\n",
    "                merged_df[f'{feat}_{suffix}_param1'] = np.nan\n",
    "                merged_df[f'{feat}_{suffix}_param2'] = np.nan\n",
    "                merged_df[f'{feat}_{suffix}_score'] = np.nan\n",
    "            continue\n",
    "\n",
    "        # --- data prep ---\n",
    "        x = (df_feat['traindate'] - start_date).dt.days.values\n",
    "        y = df_feat[feat].values\n",
    "        max_days = (end_date - start_date).days\n",
    "\n",
    "        x_uniform = np.linspace(x.min(), max_days, len(x))\n",
    "        y_interp = np.interp(x_uniform, x, y)\n",
    "\n",
    "        x_days_full = (date_range - start_date).days.values\n",
    "        original_daily = np.interp(x_days_full, x, y)\n",
    "\n",
    "        x_min, x_max = x.min(), x.max()\n",
    "\n",
    "        # =======================================================\n",
    "        # FFT HIGHPASS (short-term residual reconstruction)\n",
    "        # =======================================================\n",
    "\n",
    "        # --- objective function (maximize anomaly fidelity) ---\n",
    "        def objective_fft_highpass(cutoff):\n",
    "            n = len(x_uniform)\n",
    "            d = x_uniform[1] - x_uniform[0]\n",
    "            freqs = fftfreq(n, d=d)\n",
    "\n",
    "            Y = fft(y_interp)\n",
    "            Y_high = Y.copy()\n",
    "            Y_high[np.abs(freqs) < cutoff] = 0   # remove low freqs, keep high\n",
    "            highpass = np.real(ifft(Y_high))\n",
    "\n",
    "            highpass_daily = np.interp(x_days_full, x_uniform, highpass)\n",
    "\n",
    "            # mask outside observed range\n",
    "            highpass_daily[x_days_full < x_min] = np.nan\n",
    "            highpass_daily[x_days_full > x_max] = np.nan\n",
    "            orig_masked = original_daily.copy()\n",
    "            orig_masked[x_days_full < x_min] = np.nan\n",
    "            orig_masked[x_days_full > x_max] = np.nan\n",
    "\n",
    "            # --- evaluation metrics (only fair ones) ---\n",
    "            peak_recall, attn_ratio = eval_peak_metrics(orig_masked, highpass_daily)\n",
    "            spec_reten = spectral_energy_retention(orig_masked, highpass_daily, fs=1.0, band=(1/14,1/1))\n",
    "            fpr = false_peak_rate(orig_masked, highpass_daily)\n",
    "\n",
    "            # --- combine into a single objective ---\n",
    "            # want: HIGH recall, HIGH attenuation, HIGH spectral, LOW false peaks\n",
    "            score = (\n",
    "                + 2.0 * (peak_recall if not np.isnan(peak_recall) else 0.0)\n",
    "                + 2.0 * (attn_ratio if not np.isnan(attn_ratio) else 0.0)\n",
    "                + 0.5 * (spec_reten if not np.isnan(spec_reten) else 0.0)\n",
    "                - 1.0 * fpr\n",
    "            )\n",
    "\n",
    "            return -score  # negative because minimize_scalar minimizes\n",
    "\n",
    "\n",
    "        # --- optimize cutoff ---\n",
    "        res_fft = minimize_scalar(objective_fft_highpass, bounds=(0.0001, 0.1), method=\"bounded\")\n",
    "        best_cutoff_fft = res_fft.x\n",
    "        best_score_fft = -res_fft.fun\n",
    "\n",
    "        # --- reconstruct highpass with best cutoff ---\n",
    "        n = len(x_uniform)\n",
    "        d = x_uniform[1] - x_uniform[0]\n",
    "        freqs = fftfreq(n, d=d)\n",
    "        Y = fft(y_interp)\n",
    "        Y[np.abs(freqs) < best_cutoff_fft] = 0\n",
    "        highpass = np.real(ifft(Y))\n",
    "        best_series_fft = np.interp(x_days_full, x_uniform, highpass)\n",
    "\n",
    "        # mask outside observed range\n",
    "        best_series_fft[x_days_full < x_min] = np.nan\n",
    "        best_series_fft[x_days_full > x_max] = np.nan\n",
    "        original_daily[x_days_full < x_min] = np.nan\n",
    "        original_daily[x_days_full > x_max] = np.nan\n",
    "\n",
    "        # --- save FFT HIGHPASS result ---\n",
    "        merged_df[f'{feat}_original'] = original_daily\n",
    "        merged_df[f'{feat}_highpass'] = best_series_fft\n",
    "        merged_df[f'{feat}_highpass_param1'] = best_cutoff_fft\n",
    "        merged_df[f'{feat}_highpass_score'] = best_score_fft\n",
    "\n",
    "        # =======================================================\n",
    "        # WAVELET DENOISING / HIGHPASS RECONSTRUCTION\n",
    "        # =======================================================\n",
    "\n",
    "        # --- objective function (maximize anomaly fidelity) ---\n",
    "        def objective_wavelet_thresh(thresh):\n",
    "            # decompose with discrete wavelet transform\n",
    "            coeffs = pywt.wavedec(y_interp, wavelet=\"db4\", level=None)\n",
    "            \n",
    "            # threshold detail coefficients (remove small ones, keep larger = high freq)\n",
    "            coeffs_thresh = [coeffs[0]]  # keep approximation untouched\n",
    "            for detail in coeffs[1:]:\n",
    "                coeffs_thresh.append(pywt.threshold(detail, thresh, mode=\"hard\"))\n",
    "            \n",
    "            # reconstruct\n",
    "            y_wave = pywt.waverec(coeffs_thresh, wavelet=\"db4\")\n",
    "            \n",
    "            # align to daily grid\n",
    "            highpass_daily = np.interp(x_days_full, x_uniform, y_wave[:len(x_uniform)])\n",
    "            \n",
    "            # mask outside observed range\n",
    "            highpass_daily[x_days_full < x_min] = np.nan\n",
    "            highpass_daily[x_days_full > x_max] = np.nan\n",
    "            orig_masked = original_daily.copy()\n",
    "            orig_masked[x_days_full < x_min] = np.nan\n",
    "            orig_masked[x_days_full > x_max] = np.nan\n",
    "            \n",
    "            # --- evaluation metrics ---\n",
    "            peak_recall, attn_ratio = eval_peak_metrics(orig_masked, highpass_daily)\n",
    "            spec_reten = spectral_energy_retention(orig_masked, highpass_daily, fs=1.0, band=(1/14,1/1))\n",
    "            fpr = false_peak_rate(orig_masked, highpass_daily)\n",
    "            \n",
    "            # combine into single score\n",
    "            score = (\n",
    "                + 2.0 * (peak_recall if not np.isnan(peak_recall) else 0.0)\n",
    "                + 2.0 * (attn_ratio if not np.isnan(attn_ratio) else 0.0)\n",
    "                + 0.5 * (spec_reten if not np.isnan(spec_reten) else 0.0)\n",
    "                - 1.0 * fpr\n",
    "            )\n",
    "            \n",
    "            return -score  # negative because minimize_scalar minimizes\n",
    "\n",
    "\n",
    "        # --- optimize threshold ---\n",
    "        res_wav = minimize_scalar(objective_wavelet_thresh, bounds=(0.01, 5.0), method=\"bounded\")\n",
    "        best_thresh = res_wav.x\n",
    "        best_score_wav = -res_wav.fun\n",
    "\n",
    "        # --- reconstruct with best threshold ---\n",
    "        coeffs = pywt.wavedec(y_interp, wavelet=\"db4\", level=None)\n",
    "        coeffs_best = [coeffs[0]]\n",
    "        for detail in coeffs[1:]:\n",
    "            coeffs_best.append(pywt.threshold(detail, best_thresh, mode=\"hard\"))\n",
    "        y_wave = pywt.waverec(coeffs_best, wavelet=\"db4\")\n",
    "\n",
    "        best_series_wav = np.interp(x_days_full, x_uniform, y_wave[:len(x_uniform)])\n",
    "\n",
    "        # mask outside observed range\n",
    "        best_series_wav[x_days_full < x_min] = np.nan\n",
    "        best_series_wav[x_days_full > x_max] = np.nan\n",
    "        original_daily[x_days_full < x_min] = np.nan\n",
    "        original_daily[x_days_full > x_max] = np.nan\n",
    "\n",
    "        merged_df[f'{feat}_wavelet'] = best_series_wav\n",
    "        merged_df[f'{feat}_wavelet_param1'] = best_thresh\n",
    "        merged_df[f'{feat}_wavelet_score'] = best_score_wav\n",
    "\n",
    "        # =======================================================\n",
    "        # SSA (Singular Spectrum Analysis) HIGHPASS RECONSTRUCTION\n",
    "        # =======================================================\n",
    "\n",
    "        def _ssa_hankelize(y, L):\n",
    "            \"\"\"\n",
    "            Build the trajectory (Hankel) matrix of shape (L, K) from 1D series y.\n",
    "            \"\"\"\n",
    "            N = len(y)\n",
    "            K = N - L + 1\n",
    "            return np.column_stack([y[i:i+L] for i in range(K)])  # (L, K)\n",
    "\n",
    "        def _ssa_diagonal_averaging(X):\n",
    "            \"\"\"\n",
    "            Anti-diagonal (Hankel) averaging to go from matrix back to 1D series.\n",
    "            \"\"\"\n",
    "            L, K = X.shape\n",
    "            N = L + K - 1\n",
    "            y = np.zeros(N, dtype=float)\n",
    "            w = np.zeros(N, dtype=float)\n",
    "            for i in range(L):\n",
    "                for j in range(K):\n",
    "                    y[i + j] += X[i, j]\n",
    "                    w[i + j] += 1.0\n",
    "            return y / np.maximum(w, 1e-12)\n",
    "\n",
    "        def _ssa_lowrank_recon(y, L, r):\n",
    "            \"\"\"\n",
    "            Reconstruct the LOW-frequency (trend/seasonal) part using the first r SSA components.\n",
    "            \"\"\"\n",
    "            X = _ssa_hankelize(y, L)                      # (L, K)\n",
    "            U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "            r = max(1, min(r, len(s)))                    # safety\n",
    "            # Low-rank approximation using first r components\n",
    "            Xr = np.zeros_like(X)\n",
    "            for i in range(r):\n",
    "                Xr += s[i] * np.outer(U[:, i], Vt[i, :])\n",
    "            # Back to 1D\n",
    "            y_low = _ssa_diagonal_averaging(Xr)\n",
    "            # Trim/Pad to original length\n",
    "            return y_low[:len(y)]\n",
    "\n",
    "        # Choose an SSA window length (L). Good starting point: a bit larger than your longest \"trend\" cycle.\n",
    "        # You can set this explicitly if you like; here we pick something reasonable from the data length.\n",
    "        N = len(y_interp)\n",
    "        L_ssa = int(np.clip(round(min(max(30, N // 6), N // 2 - 1)), 2, max(2, N-1)))\n",
    "\n",
    "        # --- objective function (maximize anomaly fidelity) ---\n",
    "        def objective_ssa_rank(rank_float):\n",
    "            # rank is the number of leading components to REMOVE (i.e., model as low-freq).\n",
    "            # We'll reconstruct low-freq with first r comps, then take residual = high-pass.\n",
    "            r = int(np.clip(round(rank_float), 1, min(L_ssa, N - L_ssa + 1)))  # 1..min(L,K)\n",
    "\n",
    "            # low-frequency reconstruction and high-pass residual\n",
    "            y_low = _ssa_lowrank_recon(y_interp, L=L_ssa, r=r)\n",
    "            y_hp = y_interp - y_low\n",
    "\n",
    "            # align to daily grid\n",
    "            highpass_daily = np.interp(x_days_full, x_uniform, y_hp[:len(x_uniform)])\n",
    "\n",
    "            # mask outside observed range\n",
    "            highpass_daily[x_days_full < x_min] = np.nan\n",
    "            highpass_daily[x_days_full > x_max] = np.nan\n",
    "            orig_masked = original_daily.copy()\n",
    "            orig_masked[x_days_full < x_min] = np.nan\n",
    "            orig_masked[x_days_full > x_max] = np.nan\n",
    "\n",
    "            # --- evaluation metrics ---\n",
    "            peak_recall, attn_ratio = eval_peak_metrics(orig_masked, highpass_daily)\n",
    "            spec_reten = spectral_energy_retention(orig_masked, highpass_daily, fs=1.0, band=(1/14, 1/1))\n",
    "            fpr = false_peak_rate(orig_masked, highpass_daily)\n",
    "\n",
    "            # combine into single score (same weights as your latest wavelet block)\n",
    "            score = (\n",
    "                + 2.0 * (0.0 if np.isnan(peak_recall) else peak_recall)\n",
    "                + 1.0 * (0.0 if np.isnan(attn_ratio) else attn_ratio)\n",
    "                + 0.5 * (0.0 if np.isnan(spec_reten) else spec_reten)\n",
    "                - 1.0 * (0.0 if np.isnan(fpr) else fpr)\n",
    "            )\n",
    "            return -score  # minimize -> maximize score\n",
    "\n",
    "        # --- optimize rank (number of low-freq comps to remove) ---\n",
    "        # bounds are floats; we round inside the objective\n",
    "        max_rank = min(L_ssa, N - L_ssa + 1)  # r ≤ number of singular values available\n",
    "        res_ssa = minimize_scalar(objective_ssa_rank, bounds=(1, max_rank), method=\"bounded\")\n",
    "        best_rank = int(np.clip(round(res_ssa.x), 1, max_rank))\n",
    "        best_score_ssa = -res_ssa.fun\n",
    "\n",
    "        # --- reconstruct with best rank ---\n",
    "        y_low_best = _ssa_lowrank_recon(y_interp, L=L_ssa, r=best_rank)\n",
    "        y_hp_best = y_interp - y_low_best\n",
    "        best_series_ssa = np.interp(x_days_full, x_uniform, y_hp_best[:len(x_uniform)])\n",
    "\n",
    "        # mask outside observed range\n",
    "        best_series_ssa[x_days_full < x_min] = np.nan\n",
    "        best_series_ssa[x_days_full > x_max] = np.nan\n",
    "        original_daily[x_days_full < x_min] = np.nan\n",
    "        original_daily[x_days_full > x_max] = np.nan\n",
    "\n",
    "        # --- save SSA result ---\n",
    "        merged_df[f'{feat}_ssa'] = best_series_ssa\n",
    "        merged_df[f'{feat}_ssa_param1'] = best_rank         # number of leading SSA components removed\n",
    "        merged_df[f'{feat}_ssa_param2'] = L_ssa           # window length used (for reproducibility)\n",
    "        merged_df[f'{feat}_ssa_score'] = best_score_ssa\n",
    "\n",
    "\n",
    "    # dropping raw cols as we now have _original cols\n",
    "    #merged_df = merged_df.drop(columns=geometry_features)\n",
    "    merged_df['days'] = (pd.to_datetime(merged_df['traindate']) - pd.to_datetime(start_date)).dt.days\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de356e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_wpd = interpolate_daily_wheels(df_wpd, -1)\n",
    "#full_wpd.to_feather('wpd_trend_null.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_signal_metrics(full_wpd, geometry_features, type, param1, use_param2=False, anomaly_labels=None):\n",
    "    results = []\n",
    "\n",
    "    for feat in geometry_features:\n",
    "        orig = full_wpd[f\"{feat}_original\"].values\n",
    "        recon = full_wpd[f\"{feat}_{type}\"].values\n",
    "\n",
    "        mask = ~np.isnan(orig) & ~np.isnan(recon)\n",
    "        if mask.sum() < 3:\n",
    "            metrics = dict(\n",
    "                peak_recall=np.nan,\n",
    "                attenuation_ratio=np.nan,\n",
    "                spectral_retention=np.nan,\n",
    "                false_peak_rate=np.nan\n",
    "            )\n",
    "        else:\n",
    "            # Peak recall & attenuation\n",
    "            peak_recall, attn_ratio = eval_peak_metrics(orig[mask], recon[mask])\n",
    "\n",
    "            # Spectral energy retention (example band: 1–14 days)\n",
    "            spec_reten = spectral_energy_retention(orig[mask], recon[mask], fs=1.0, band=(1/14,1/1))\n",
    "\n",
    "            # False peak rate\n",
    "            fpr = false_peak_rate(orig[mask], recon[mask])\n",
    "\n",
    "            metrics = dict(\n",
    "                peak_recall=peak_recall,\n",
    "                attenuation_ratio=attn_ratio,\n",
    "                spectral_retention=spec_reten,\n",
    "                false_peak_rate=fpr\n",
    "            )\n",
    "\n",
    "        # param1 always present\n",
    "        param1_val = full_wpd[f\"{feat}_{type}_param1\"].dropna().mean()\n",
    "        record = {\"feature\": feat, \"param1\": param1_val}\n",
    "\n",
    "        if use_param2:\n",
    "            param2_val = full_wpd[f\"{feat}_{type}_param2\"].dropna().mean()\n",
    "            record[\"param2\"] = param2_val\n",
    "\n",
    "        record.update(metrics)\n",
    "        results.append(record)\n",
    "\n",
    "    return pd.DataFrame(results).set_index(\"feature\")\n",
    "\n",
    "\n",
    "for type in [\"highpass\",\"wavelet\"]:\n",
    "    summary = compute_signal_metrics(full_wpd, geometry_features, type, \"param1\")\n",
    "    display(summary)   # pretty df display\n",
    "\n",
    "summary = compute_signal_metrics(full_wpd, geometry_features, \"ssa\", \"param1\", use_param2=True)\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d74a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "temp = full_wpd[(full_wpd['equipmentnumber'] == 1) & (full_wpd['axle'] == 1) & (full_wpd['side'] == 'L')]['applieddate'].unique()\n",
    "temp = full_wpd[(full_wpd['equipmentnumber'] == 1) & (full_wpd['axle'] == 1) & (full_wpd['side'] == 'L') & (full_wpd['applieddate'] == temp[1])]\n",
    "# make sure we have days (if not already in full_wpd)\n",
    "if \"days\" not in temp.columns:\n",
    "    temp[\"days\"] = (temp[\"traindate\"] - temp[\"traindate\"].min()).dt.days\n",
    "\n",
    "signal = \"kalman\"\n",
    "# find all features that have a lowpass\n",
    "features = [c.replace(f\"_{signal}\", \"\") for c in temp.columns if c.endswith(f\"_{signal}\")]\n",
    "\n",
    "for feat in features:\n",
    "    raw_col   = f\"{feat}\"\n",
    "    signal_col    = f\"{feat}_{signal}\"\n",
    "\n",
    "    if raw_col not in temp or signal_col not in temp:\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(temp[\"days\"], temp[raw_col], label=\"Raw\", alpha=0.3)\n",
    "    plt.plot(temp[\"days\"], temp[signal_col], color='orange', label=signal.capitalize())\n",
    "\n",
    "    plt.title(f\"{feat} - Raw vs {signal.capitalize()}\")\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e76527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
